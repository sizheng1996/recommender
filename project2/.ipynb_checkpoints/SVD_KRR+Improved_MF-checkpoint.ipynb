{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "import csv\n",
    "from numpy import linalg as LA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Note that `ratings` is a sparse matrix that in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "(10000, 1000)\n",
      "r37_c1\n",
      "(37, 1)\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "path_dataset = \"47b05e70-6076-44e8-96da-2530dc2187de_data_train.csv\"\n",
    "path_submission = \"9b4d32bb-f99a-466f-95a1-0ab80048971c_sample_submission (2).csv\"\n",
    "ratings = load_data(path_dataset)\n",
    "submission = load_submission(path_submission)\n",
    "submission_row_col = submission[0]\n",
    "submission_pos = submission[1]\n",
    "print(ratings.shape)\n",
    "print(submission_pos[0])\n",
    "print(submission_row_col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_sub    number of users:  1000 number of items:  10000\n"
     ]
    }
   ],
   "source": [
    "ratings_sub = ratings.T\n",
    "print(\"ratings_sub   \",\"number of users: \",ratings_sub.shape[0],\"number of items: \",ratings_sub.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the 500 most frequently rated Movie per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_num_users_per_item = np.array((ratings_sub != 0).sum(axis=0))[0]\n",
    "array_num_items_per_user = np.array((ratings_sub != 0).sum(axis=1).T)[0]\n",
    "\n",
    "# (num_users_per_item/num_items_per_user,ind) \n",
    "# ind = position in the original ratings matrix\n",
    "# nupi = number of users per item\n",
    "# nipu = number of items per user\n",
    "list_of_tuple_nupi = []\n",
    "list_of_tuple_nipu = []\n",
    "for ind in range(len(array_num_users_per_item)):\n",
    "    list_of_tuple_nupi.append((array_num_users_per_item[ind],ind))\n",
    "for ind in range(len(array_num_items_per_user)):\n",
    "    list_of_tuple_nipu.append((array_num_items_per_user[ind],ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_frequently_rated(list_of_tuple_nupi,user,ratings_sub):\n",
    "    list_items_rated = []\n",
    "    list_most_rated_col = []\n",
    "    items_rated = (ratings_sub[user,:] != 0).toarray()\n",
    "    items_rated = np.where(items_rated != 0 )[1]\n",
    "    for item in items_rated:\n",
    "        list_items_rated.append(list_of_tuple_nupi[item])\n",
    "    list_items_rated.sort()\n",
    "    list_items_rated.reverse()\n",
    "    num_items_rated = len(list_items_rated) \n",
    "    if num_items_rated > 500:\n",
    "        num_of_most = 500\n",
    "    else:\n",
    "        num_of_most = num_items_rated\n",
    "    for i in range(num_of_most):\n",
    "        num,col = list_items_rated[i]\n",
    "        list_most_rated_col.append(col)\n",
    "    return list_most_rated_col, num_items_rated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1,x2):\n",
    "    return np.exp(2 * (x1.dot(x2.T) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization of improved MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "        \n",
    "    num_item, num_user = train.get_shape()\n",
    "\n",
    "    user_features = np.random.rand(num_features, num_user)\n",
    "    item_features = np.random.rand(num_features, num_item)\n",
    "    user_bias = np.random.rand(num_user)\n",
    "    item_bias = np.random.rand(num_item)\n",
    "\n",
    "    # start by item features.\n",
    "    item_nnz = train.getnnz(axis=1)\n",
    "    item_sum = train.sum(axis=1)\n",
    "\n",
    "    for ind in range(num_item):\n",
    "        item_features[0, ind] = item_sum[0, ind] / item_nnz[ind]\n",
    "        \n",
    "    # then user features.\n",
    "    user_nnz = train.getnnz(axis=0)\n",
    "    user_sum = train.sum(axis=0)\n",
    "    \n",
    "    for ind in range(num_user):\n",
    "        user_features[0, ind] = user_sum[ind, 0] / user_nnz[ind]\n",
    "        \n",
    "    # intialize bias\n",
    "    user_bias = user_features[0,:]\n",
    "    item_bias = item_features[0,:]\n",
    "        \n",
    "    return user_features, item_features, user_bias, item_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 1000) (96, 10000)\n",
      "learn the matrix factorization using SGD...\n"
     ]
    }
   ],
   "source": [
    "def matrix_factorization_SGD(train):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = 0.01\n",
    "    num_features = 96   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    num_epochs = 20     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    print(user_features.shape,item_features.shape)\n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for n, d in nz_train:\n",
    "            # update W_d (item_features[:, d]) and Z_n (user_features[:, n])\n",
    "            item_info = item_features[:, d]\n",
    "            user_info = user_features[:, n]\n",
    "            item_deviation = item_bias[d]\n",
    "            user_deviation = user_bias[n]\n",
    "            err = train[d, n] - user_info.T.dot(item_info) - item_deviation - user_deviation\n",
    "    \n",
    "            # calculate the gradient and update\n",
    "            item_features[:, d] += gamma * (err * user_info - lambda_item * item_info)\n",
    "            user_features[:, n] += gamma * (err * item_info - lambda_user * user_info)\n",
    "            item_bias[d] += gamma * (err  - lambda_item * (item_deviation + user_deviation - global_mean))\n",
    "            user_bias[n] += gamma * (err  - lambda_item * (item_deviation + user_deviation - global_mean))\n",
    "    \n",
    "    # output the resulting matrices\n",
    "    return user_features, item_features, user_bias, item_bias      \n",
    "\n",
    "user_features, item_features, user_bias, item_bias = matrix_factorization_SGD(train, test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression using item features （submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_ridge_regression(user, item_features, ratings_sub):\n",
    "    list_most_rated_col,num_of_most = select_most_frequently_rated(list_of_tuple_nupi,user,ratings_sub)\n",
    "    y = ratings_sub[user,list_most_rated_col]\n",
    "    x = item_features.T[list_most_rated_col,:]\n",
    "    \n",
    "    # Normalize rows in x\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i,:] = x[i,:] / LA.norm(x[i,:])\n",
    "    return x,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记得-1\n",
    "def predict(submission_row_col, ratings_sub, item_features,lambda_):\n",
    "    num_user, num_item = ratings_sub.get_shape()\n",
    "    predictions = []\n",
    "    \n",
    "    item_old,user_old = submission_row_col[0]\n",
    "    user_old,item_old = user_old - 1, item_old - 1\n",
    "    x,y = kernel_ridge_regression(user_old,item_features, ratings_sub)\n",
    "    y = y.toarray()\n",
    "    for item,user in submission_row_col:\n",
    "        user,item = user - 1, item - 1\n",
    "        \n",
    "        if user != user_old:        \n",
    "            x,y = kernel_ridge_regression(user, item_features, ratings_sub)\n",
    "            y = y.toarray()\n",
    "            \n",
    "        item_feature = item_features[:,item].T\n",
    "        item_feature = item_feature / LA.norm(item_feature)\n",
    "        normal = gaussian_kernel(x,x) + lambda_ * np.eye(x.shape[0])\n",
    "        ridge = LA.solve(normal,y.T)\n",
    "        value_pred = gaussian_kernel(item_feature,x).dot(ridge)\n",
    "        value_pred = round(float(value_pred))\n",
    "        if value_pred > 5:\n",
    "            value_pred = 5\n",
    "        elif value_pred < 1:\n",
    "            value_pred = 1\n",
    "        predictions.append(value_pred)\n",
    "        user_old = user\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pred = predict(submission_row_col, ratings_sub, item_features, 0.5)\n",
    "create_csv_submission(submission_pos, ratings_pred, \"pred_kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local RMSE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # init\n",
    "    num_rows, num_cols = valid_ratings.shape\n",
    "    train = sp.lil_matrix((num_rows, num_cols))\n",
    "    test = sp.lil_matrix((num_rows, num_cols))\n",
    "    \n",
    "    print(\"the shape of original ratings. (# of row, # of col): {}\".format(\n",
    "        ratings.shape))\n",
    "    print(\"the shape of valid ratings. (# of row, # of col): {}\".format(\n",
    "        (num_rows, num_cols)))\n",
    "\n",
    "    nz_items, nz_users = valid_ratings.nonzero()\n",
    "    \n",
    "    # split the data\n",
    "    for user in set(nz_users):\n",
    "        # randomly select a subset of ratings\n",
    "        row, col = valid_ratings[:, user].nonzero()\n",
    "        selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "        residual = list(set(row) - set(selects))\n",
    "\n",
    "        # add to train set\n",
    "        train[residual, user] = valid_ratings[residual, user]\n",
    "\n",
    "        # add to test set\n",
    "        test[selects, user] = valid_ratings[selects, user]\n",
    "\n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    train = train.T\n",
    "    test = test.T\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (9990, 999)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1065253\n",
      "Total number of nonzero elements in test data:111620\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of SVD_KNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "        \n",
    "    num_user, num_item = train.get_shape()\n",
    "\n",
    "    user_features = np.random.rand(num_features, num_user)\n",
    "    item_features = np.random.rand(num_features, num_item)\n",
    "\n",
    "    # start by item features.\n",
    "    item_nnz = train.getnnz(axis=0)\n",
    "    item_sum = train.sum(axis=0)\n",
    "\n",
    "    for ind in range(num_item):\n",
    "        item_features[0, ind] = item_sum[0, ind] / item_nnz[ind]\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 999) (96, 9990)\n",
      "learn the matrix factorization using SGD...\n"
     ]
    }
   ],
   "source": [
    "user_features, item_features = matrix_factorization_SGD(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_error:  lambda =  5 1.0845876666905003\n"
     ]
    }
   ],
   "source": [
    "def predict_error_compute(seuil, test, train, user_features, item_features, lambda_, user_bias, item_bias):\n",
    "    mse = 0\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    for user, item in nz_test:\n",
    "        list_most_rated_col, num_items_rated = select_most_frequently_rated(list_of_tuple_nupi,user,train)\n",
    "        \n",
    "        # If the number of items rated of an user is greater than the seuil(the data have sufficient information) we use SVD_KNN\n",
    "        if num_items_rated > seuil:\n",
    "            \n",
    "            x,y = kernel_ridge_regression(user,item_features, train)\n",
    "            y = y.toarray()\n",
    "            \n",
    "            item_feature = item_features[:,item].T\n",
    "            item_feature = item_feature / LA.norm(item_feature)\n",
    "            normal = gaussian_kernel(x,x) + lambda_ * np.eye(x.shape[0])\n",
    "            ridge = LA.solve(normal,y.T)\n",
    "            value_pred = gaussian_kernel(item_feature,x).dot(ridge)\n",
    "        \n",
    "        # Else we use improved_MF\n",
    "        else:\n",
    "            item_info = item_features[:, row]\n",
    "            user_info = user_features[:, col]\n",
    "            item_deviation = item_bias[row]\n",
    "            user_deviation = user_bias[col]\n",
    "            value_pred = user_info.T.dot(item_info) + item_deviation + user_deviation\n",
    "            \n",
    "        value_pred = round(float(value_pred))\n",
    "        if value_pred > 5:\n",
    "            value_pred = 5\n",
    "#        print(value_pred,' ',test[user,item])\n",
    "        mse += (test[user, item] - value_pred) ** 2\n",
    "    return np.sqrt(1.0 * mse / len(nz_test))\n",
    "\n",
    "lambda_ = 5\n",
    "\n",
    "test_error = predict_error_compute(test, train, item_features,lambda_)\n",
    "print(\"test_error: \", \"lambda = \",lambda_, test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
