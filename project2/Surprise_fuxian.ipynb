{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "import csv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "path_dataset = \"47b05e70-6076-44e8-96da-2530dc2187de_data_train.csv\"\n",
    "path_submission = \"9b4d32bb-f99a-466f-95a1-0ab80048971c_sample_submission (2).csv\"\n",
    "ratings = load_data(path_dataset)\n",
    "ratings_=ratings.toarray()\n",
    "submission = load_submission(path_submission)\n",
    "submission_row_col = submission[0]\n",
    "submission_pos = submission[1]\n",
    "num_item, num_user = ratings.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nzi = len(ratings.nonzero()[0])\n",
    "nb_nzu = len(ratings.nonzero()[1])\n",
    "\n",
    "users = ratings.nonzero()[1]\n",
    "items = ratings.nonzero()[0]\n",
    "\n",
    "stars = []\n",
    "for i in range(nb_nzi):\n",
    "    stars.append(ratings[items[i],users[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('surprise', 'w') as csvfile:\n",
    "    fieldnames = ['item', 'user','ratings']\n",
    "    writer = csv.DictWriter(csvfile, delimiter=\";\", fieldnames=fieldnames)\n",
    "    for r1, r2,r3 in zip(items, users, stars):\n",
    "        writer.writerow({'item':r1,'user':r2,'ratings':r3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_mean(train):\n",
    "    #calculate user mean\n",
    "    user_nnz = train.getnnz(axis=0)\n",
    "    user_sum = train.sum(axis=0)\n",
    "    user_mean = np.empty((1, num_user))\n",
    "    for ind in range(num_user):\n",
    "        user_mean[0,ind] = user_sum[0,ind] / user_nnz[ind]\n",
    "    return user_mean\n",
    "def extract_global_mean(train):\n",
    "    # calculate the global mean\n",
    "    nonzero_train = train[train.nonzero()]\n",
    "    global_mean = nonzero_train.mean()\n",
    "    return global_mean\n",
    "user_mean=extract_user_mean(ratings)\n",
    "global_mean=extract_global_mean(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_estimate(train,lamda_i,lamda_u,epochs):\n",
    "    # set the user and item baselines\n",
    "    bu = np.zeros(num_user)\n",
    "    bi = np.zeros(num_item)    \n",
    "    \n",
    "    # group the indices by row or column index\n",
    "    nz_train, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "    \n",
    "    #using Alternating Least Squares (ALS)\n",
    "    for iter_ in range(epochs):\n",
    "        for i,i_users in nz_item_userindices:\n",
    "            dev_i = 0\n",
    "            for u in i_users:\n",
    "                dev_i += train[i,u] - global_mean - bu[u]\n",
    "\n",
    "            bi[i] = dev_i / (lamda_i + len(i_users))\n",
    "\n",
    "        for u,u_items in nz_user_itemindices:\n",
    "            dev_u = 0    \n",
    "            for i in u_items:\n",
    "                dev_u += train[i,u] - global_mean - bi[i]\n",
    "\n",
    "            bu[u] = dev_u / (lamda_u + len(u_items))\n",
    "   \n",
    "    return bu,bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "lamda_i = 10\n",
    "lamda_u = 15\n",
    "epochs = 10\n",
    "#baseline_estimate\n",
    "bu,bi = baseline_estimate(ratings,lamda_i,lamda_u,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity with pearson baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_similarity_by_pearson_baseline(train,min_support,global_mean, user_biases, item_biases, shrinkage=100):\n",
    "    train=train.toarray()\n",
    "    # set some matrixs\n",
    "    freq = np.zeros((num_user, num_user))# matrix of number of common items\n",
    "    prods = np.zeros((num_user, num_user))# matrix of sum (r_ui - b_ui) * (r_vi - b_vi) for common items\n",
    "    sq_diff_u = np.zeros((num_user,num_user))# matrix of sum (r_ui - b_ui)**2 for common items\n",
    "    sq_diff_v = np.zeros((num_user,num_user))# matrix of sum (r_vi - b_vi)**2 for common items\n",
    "    sim = np.zeros((num_user, num_user))#matrix of similatiries\n",
    "\n",
    "    # Need this because of shrinkage. When pearson coeff is zero when support is 1, so that's OK.\n",
    "    min_support = max(2, min_support)\n",
    "\n",
    "    # group the indices by row or column index\n",
    "    nz_train, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "    \n",
    "    for u,items_u in nz_user_itemindices:\n",
    "        sim[u, u] = 1\n",
    "        for v,items_v in nz_user_itemindices[(u+1):]:  \n",
    "            com_items = np.intersect1d(items_u,items_v)\n",
    "            freq[u, v] = len(com_items)\n",
    "            diff_u = (train[com_items,u] - (global_mean + item_biases[com_items] + user_biases[u]))\n",
    "            diff_v = (train[com_items,v] - (global_mean + item_biases[com_items] + user_biases[v]))\n",
    "            prods[u, v]= diff_u.T @ diff_v\n",
    "            sq_diff_u[u, v] = diff_u.T @ diff_u\n",
    "            sq_diff_v[u, v] = diff_v.T @ diff_v\n",
    "            if freq[u, v] < min_support:\n",
    "                sim[u, v] = 0\n",
    "            else:\n",
    "                # calculate the similarity\n",
    "                sim[u, v] = prods[u, v] / (np.sqrt(sq_diff_u[u, v] *\n",
    "                                                       sq_diff_v[u, v]))\n",
    "                # shrunk similarity\n",
    "                sim[u, v] *= (freq[u, v] - 1) / (freq[u, v] - 1 +\n",
    "                                                     shrinkage)\n",
    "\n",
    "            sim[v, u] = sim[u, v]\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the parameters\n",
    "min_support = 1\n",
    "shrinkage = 1000\n",
    "sim = user_based_similarity_by_pearson_baseline(ratings, min_support, global_mean, bu, bi, shrinkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_with_user_means(train,sim_matrix,k,min_k,user_mean):\n",
    "    \n",
    "    pred=[]\n",
    "    \n",
    "    for row,col in submission_row_col:\n",
    "        i = row-1\n",
    "        u = col-1\n",
    "        #x, y = self.switch(u, i)neighbors=[]\n",
    "        neighbors=[]\n",
    "        for v in range(num_user):\n",
    "            if train[i,v]>0:\n",
    "                new_neighbors=(v,sim_matrix[u, v],train[i,v])\n",
    "                neighbors.append(new_neighbors)\n",
    "        # Extract the top-K most-similar ratings\n",
    "        k_neighbors = heapq.nlargest(k, neighbors, key=lambda t: t[1])\n",
    "\n",
    "        #initial setting\n",
    "        est = user_mean[u]\n",
    "        sum_sim = 0\n",
    "        sum_ratings = 0\n",
    "        actual_k = 0\n",
    "        \n",
    "        # compute weighted average\n",
    "        for (nb,sim_, r) in k_neighbors:\n",
    "            if sim_ > 0:\n",
    "                sum_sim += sim_\n",
    "                sum_ratings += (sim_ * (r - user_mean[nb]) )\n",
    "                actual_k += 1\n",
    "\n",
    "        if actual_k < min_k:\n",
    "            sum_ratings = 0\n",
    "        if sum_sim>0:\n",
    "            est += sum_ratings / sum_sim\n",
    "        \n",
    "        # round ratings\n",
    "        if est < 1:\n",
    "            est = 1\n",
    "        elif est > 5:\n",
    "            est = 5\n",
    "        else:\n",
    "            est = np.round(est)\n",
    "        pred.append(est)         \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "#initial setting    \n",
    "k = 200\n",
    "min_k = 1\n",
    "ratings_ = ratings.toarray()\n",
    "pred = KNN_with_user_means(ratings_,sim,k,min_k,user_mean[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(submission_pos, pred, \"pred_fuxian.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
